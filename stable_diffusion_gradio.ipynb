{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q diffusers transformers xformers accelerate ray safetensors\n",
    "# %pip install -q numpy scipy ftfy imageio matplotlib Pillow gradio\n",
    "# %pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "# your models cache will be stored here\n",
    "# os.environ['HUGGINGFACE_HUB_CACHE'] = 'D:\\\\Code\\\\Huggingface_cache\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline, \n",
    "    StableDiffusionInpaintPipeline, \n",
    "    StableDiffusionImg2ImgPipeline, \n",
    "    CycleDiffusionPipeline, \n",
    "    StableDiffusionDepth2ImgPipeline\n",
    ")\n",
    "\n",
    "from diffusers import (\n",
    "    DDIMScheduler, \n",
    "    PNDMScheduler, \n",
    "    LMSDiscreteScheduler, \n",
    "    DPMSolverMultistepScheduler, \n",
    "    EulerAncestralDiscreteScheduler, \n",
    "    EulerDiscreteScheduler\n",
    ")\n",
    "\n",
    "\n",
    "from accelerate.hooks import remove_hook_from_submodules\n",
    "\n",
    "from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimization https://huggingface.co/docs/diffusers/optimization/fp16#memory-efficient-attention\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token: https://huggingface.co/settings/tokens\n",
    "HF_TOKEN = \"YOUR_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\" #\"ckpt/anything-v4.5\"#\"andite/anything-v4.0\"#\"SG161222/Realistic_Vision_V1.4\" #\"Linaqruf/anything-v3.0\" #\"admruul/anything-v3.0\" #\"stabilityai/stable-diffusion-2-1\"  #\"CompVis/stable-diffusion-v1-4\" #\"hakurei/waifu-diffusion\" #\"runwayml/stable-diffusion-v1-5\"\n",
    "inpaint_model_id = \"runwayml/stable-diffusion-inpainting\"\n",
    "depth2img_model_id = \"stabilityai/stable-diffusion-2-depth\"\n",
    "# model_id = \"D:\\\\Code\\\\Huggingface_cache\\\\800\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)   \n",
    "class StableDiffusionInterface(): \n",
    "    def __init__(self, model_name, scheduler, use_auth_token = None, torch_dtype = torch.float32, safe_mode = False, device = \"cuda\", revision = \"fp16\",):\n",
    "        self.pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            model_name, \n",
    "            scheduler = scheduler,  \n",
    "            torch_dtype = torch_dtype,  \n",
    "            use_auth_token = use_auth_token,\n",
    "        )\n",
    "        self.pipe.safety_checker = None\n",
    "\n",
    "        #optimization\n",
    "        self.pipe.enable_model_cpu_offload()\n",
    "        self.pipe.enable_attention_slicing(1)\n",
    "        self.pipe.unet.to(memory_format=torch.channels_last)\n",
    "        self.pipe.enable_vae_slicing()\n",
    "        self.pipe.enable_vae_tiling()\n",
    "        self.pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.pipe.disable_xformers_memory_efficient_attention()\n",
    "        self.pipe.disable_attention_slicing()\n",
    "        remove_hook_from_submodules(self.pipe.vae)\n",
    "        remove_hook_from_submodules(self.pipe.text_encoder)\n",
    "        remove_hook_from_submodules(self.pipe.unet)\n",
    "        \n",
    "    def set_scheduler(self, scheduler):\n",
    "        self.pipe.scheduler = scheduler\n",
    "\n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "    def model_name(self):\n",
    "        return self.model_name\n",
    "   \n",
    "    def __call__(\n",
    "        self, \n",
    "        prompt = \"\", \n",
    "        height = 64, \n",
    "        width = 64, \n",
    "        negative_prompt = \"\", \n",
    "        num_images_per_prompt = 1, \n",
    "        num_inference_steps = 50, \n",
    "        guidance_scale = 7.5, \n",
    "        seed = None\n",
    "    ):\n",
    "        \n",
    "        g_cuda = None\n",
    "        if seed is not None:\n",
    "            g_cuda = torch.Generator(device='cuda')\n",
    "            g_cuda.manual_seed(seed)\n",
    "\n",
    "        return self.pipe(\n",
    "            prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator = g_cuda\n",
    "        ).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)   \n",
    "class CycleDiffusionInterface:\n",
    "    def __init__(self, model_name, scheduler, use_auth_token = None, torch_dtype = torch.float32, safe_mode = False, device = \"cuda\", revision = \"fp16\",):\n",
    "        self.pipe = CycleDiffusionPipeline.from_pretrained(\n",
    "            model_name, \n",
    "            scheduler = scheduler,  \n",
    "            torch_dtype=torch_dtype,  \n",
    "            use_auth_token = use_auth_token,\n",
    "            # revision=revision, \n",
    "        )\n",
    "        self.pipe.safety_checker = None\n",
    "\n",
    "        self.pipe.enable_model_cpu_offload()\n",
    "        self.pipe.enable_attention_slicing(1)\n",
    "        self.pipe.unet.to(memory_format=torch.channels_last)\n",
    "        # self.pipe.enable_vae_slicing() #Not working with cycle stable diffusion pipeline\n",
    "        # self.pipe.enable_vae_tiling() #Not working with cycle stable diffusion pipeline\n",
    "        self.pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.pipe.disable_xformers_memory_efficient_attention()\n",
    "        self.pipe.disable_attention_slicing()\n",
    "        remove_hook_from_submodules(self.pipe.vae)\n",
    "        remove_hook_from_submodules(self.pipe.text_encoder)\n",
    "        remove_hook_from_submodules(self.pipe.unet)\n",
    "\n",
    "    def set_scheduler(self, scheduler):\n",
    "        self.pipe.scheduler = scheduler\n",
    "\n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "    def model_name(self):\n",
    "        return self.model_name\n",
    "    \n",
    "    def __call__(\n",
    "        self, \n",
    "        prompt = \"\",  \n",
    "        source_prompt = \"\", \n",
    "        image = None, \n",
    "        height = 64, \n",
    "        width = 64,  \n",
    "        num_images_per_prompt = 1, \n",
    "        num_inference_steps = 50, \n",
    "        eta=0.1,\n",
    "        strength=0.85,\n",
    "        guidance_scale = 7.5, \n",
    "        source_guidance_scale=1,\n",
    "        seed = None\n",
    "    ):\n",
    "        \n",
    "        g_cuda = None\n",
    "        if seed is not None:\n",
    "            g_cuda = torch.Generator(device='cuda')\n",
    "            g_cuda.manual_seed(seed)\n",
    "\n",
    "        return self.pipe(\n",
    "            prompt,\n",
    "            source_prompt = source_prompt,\n",
    "            image = image.resize((width, height)),\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            eta=eta,\n",
    "            strength=strength,\n",
    "            guidance_scale=guidance_scale,\n",
    "            source_guidance_scale=source_guidance_scale,\n",
    "            generator = g_cuda\n",
    "        ).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)   \n",
    "class Img2ImgInterface:\n",
    "    def __init__(self, model_name, scheduler, use_auth_token = None, torch_dtype = torch.float32, safe_mode = False, device = \"cuda\", revision = \"fp16\",):\n",
    "        self.pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "            model_name, \n",
    "            scheduler = scheduler,  \n",
    "            torch_dtype=torch_dtype,  \n",
    "            use_auth_token = use_auth_token,\n",
    "            # revision=revision, \n",
    "        )\n",
    "        self.pipe.safety_checker = None\n",
    "\n",
    "        self.pipe.enable_model_cpu_offload()\n",
    "        self.pipe.enable_attention_slicing(1)\n",
    "        self.pipe.unet.to(memory_format=torch.channels_last)\n",
    "        # self.pipe.enable_vae_slicing() #Not working with img2img stable diffusion pipeline\n",
    "        # self.pipe.enable_vae_tiling() #Not working with img2img stable diffusion pipeline\n",
    "        self.pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.pipe.disable_xformers_memory_efficient_attention()\n",
    "        self.pipe.disable_attention_slicing()\n",
    "        remove_hook_from_submodules(self.pipe.vae)\n",
    "        remove_hook_from_submodules(self.pipe.text_encoder)\n",
    "        remove_hook_from_submodules(self.pipe.unet)\n",
    "\n",
    "    def set_scheduler(self, scheduler):\n",
    "        self.pipe.scheduler = scheduler\n",
    "\n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "    def model_name(self):\n",
    "        return self.model_name\n",
    "    \n",
    "    def __call__(\n",
    "        self, \n",
    "        prompt = \"\",  \n",
    "        negative_prompt = \"\", \n",
    "        image = None, \n",
    "        height = 64, \n",
    "        width = 64,  \n",
    "        num_images_per_prompt = 1, \n",
    "        num_inference_steps = 50, \n",
    "        eta=0.1,\n",
    "        strength=0.85,\n",
    "        guidance_scale = 7.5, \n",
    "        seed = None\n",
    "    ):\n",
    "        \n",
    "        g_cuda = None\n",
    "        if seed is not None:\n",
    "            g_cuda = torch.Generator(device='cuda')\n",
    "            g_cuda.manual_seed(seed)\n",
    "\n",
    "        return self.pipe(\n",
    "            prompt,\n",
    "            negative_prompt = negative_prompt,\n",
    "            image = image.resize((width, height)),\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            eta=eta,\n",
    "            strength=strength,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator = g_cuda\n",
    "        ).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)   \n",
    "class InpaintInterface:\n",
    "    def __init__(self, model_name, scheduler, use_auth_token = None, torch_dtype = torch.float32, safe_mode = False, device = \"cuda\", revision = \"fp16\"):\n",
    "        self.pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "            model_name, \n",
    "            scheduler = scheduler,  \n",
    "            torch_dtype=torch_dtype,  \n",
    "            use_auth_token = use_auth_token,\n",
    "        )\n",
    "        self.pipe.safety_checker = None\n",
    "\n",
    "        self.pipe.enable_model_cpu_offload()\n",
    "        self.pipe.enable_attention_slicing(1)\n",
    "        self.pipe.unet.to(memory_format=torch.channels_last)\n",
    "        # self.pipe.enable_vae_slicing() #Not working with inpaint stable diffusion pipeline\n",
    "        # self.pipe.enable_vae_tiling() #Not working with inpaint stable diffusion pipeline\n",
    "        self.pipe.enable_xformers_memory_efficient_attention()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        self.pipe.disable_xformers_memory_efficient_attention()\n",
    "        self.pipe.disable_attention_slicing()\n",
    "        remove_hook_from_submodules(self.pipe.vae)\n",
    "        remove_hook_from_submodules(self.pipe.text_encoder)\n",
    "        remove_hook_from_submodules(self.pipe.unet)\n",
    "\n",
    "    def set_scheduler(self, scheduler):\n",
    "        self.pipe.scheduler = scheduler\n",
    "        \n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "    def model_name(self):\n",
    "        return self.model_name\n",
    "    \n",
    "    def __call__(\n",
    "        self, \n",
    "        prompt = \"\",  \n",
    "        negative_prompt = \"\", \n",
    "        image = None, \n",
    "        mask_image = None,\n",
    "        height = 64, \n",
    "        width = 64,  \n",
    "        num_images_per_prompt = 1, \n",
    "        num_inference_steps = 50, \n",
    "        eta=0.1,\n",
    "        guidance_scale = 7.5, \n",
    "        seed = None\n",
    "    ):\n",
    "        \n",
    "        g_cuda = None\n",
    "        if seed is not None:\n",
    "            g_cuda = torch.Generator(device='cuda')\n",
    "            g_cuda.manual_seed(seed)\n",
    "\n",
    "        return self.pipe(\n",
    "            prompt,\n",
    "            negative_prompt = negative_prompt,\n",
    "            image = image.resize((width, height)),\n",
    "            mask_image = mask_image.resize((width, height)),\n",
    "            height = height,\n",
    "            width = width,\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            eta=eta,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator = g_cuda\n",
    "        ).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)   \n",
    "class Depth2ImgInterface:\n",
    "    def __init__(self, model_name, scheduler, use_auth_token = None, torch_dtype = torch.float32, safe_mode = False, device = \"cuda\", revision = \"fp16\"):\n",
    "        self.pipe = StableDiffusionDepth2ImgPipeline.from_pretrained(\n",
    "            model_name, \n",
    "            scheduler = scheduler,  \n",
    "            torch_dtype=torch_dtype,  \n",
    "            use_auth_token = use_auth_token,\n",
    "        )\n",
    "        self.pipe.safety_checker = None\n",
    "\n",
    "        # self.pipe.enable_model_cpu_offload()\n",
    "        self.pipe.enable_sequential_cpu_offload()\n",
    "        self.pipe.enable_attention_slicing(1)\n",
    "        self.pipe.unet.to(memory_format=torch.channels_last)\n",
    "        # self.pipe.enable_vae_slicing() #Not working with upscale stable diffusion pipeline\n",
    "        # self.pipe.enable_vae_tiling() #Not working with upscale stable diffusion pipeline\n",
    "        self.pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "        self.model_name = model_name\n",
    "\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        self.pipe.disable_xformers_memory_efficient_attention()\n",
    "        self.pipe.disable_attention_slicing()\n",
    "        remove_hook_from_submodules(self.pipe.vae)\n",
    "        remove_hook_from_submodules(self.pipe.text_encoder)\n",
    "        remove_hook_from_submodules(self.pipe.unet)\n",
    "\n",
    "    def set_scheduler(self, scheduler):\n",
    "        self.pipe.scheduler = scheduler\n",
    "        \n",
    "    def name(self):\n",
    "        return self.__class__.__name__\n",
    "    \n",
    "    def model_name(self):\n",
    "        return self.model_name\n",
    "    \n",
    "    def __call__(\n",
    "        self, \n",
    "        prompt = \"\",  \n",
    "        negative_prompt = \"\", \n",
    "        image = None, \n",
    "        height = 64, \n",
    "        width = 64,  \n",
    "        num_images_per_prompt = 1, \n",
    "        num_inference_steps = 50, \n",
    "        eta=0.1,\n",
    "        strength=0.85,\n",
    "        guidance_scale = 7.5, \n",
    "        seed = None\n",
    "    ):\n",
    "        \n",
    "        g_cuda = None\n",
    "        if seed is not None:\n",
    "            g_cuda = torch.Generator(device='cuda')\n",
    "            g_cuda.manual_seed(seed)\n",
    "\n",
    "        return self.pipe(\n",
    "            prompt,\n",
    "            negative_prompt = negative_prompt,\n",
    "            image = image.resize((width, height)),\n",
    "            num_images_per_prompt=num_images_per_prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            eta=eta,\n",
    "            strength=strength,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator = g_cuda\n",
    "        ).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schedulers for the diffusion process.\n",
    "schedulers = {\n",
    "    \"EulerAncestralDiscreteScheduler\": EulerAncestralDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\"),\n",
    "    \"EulerDiscreteScheduler\": EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\"),\n",
    "    \"DDIMScheduler\": DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\"),\n",
    "    \"DPMSolverMultistepScheduler\": DPMSolverMultistepScheduler.from_pretrained(model_id, subfolder=\"scheduler\"),\n",
    "    \"LMSDiscreteScheduler\": LMSDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\"),\n",
    "    \"PNDMScheduler\": PNDMScheduler.from_pretrained(model_id, subfolder=\"scheduler\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_diffusion_interface = StableDiffusionInterface.remote(\n",
    "    model_id,\n",
    "    scheduler = schedulers[\"EulerAncestralDiscreteScheduler\"],  \n",
    "    use_auth_token = HF_TOKEN,\n",
    ")\n",
    "\n",
    "# stable_diffusion_interface = Img2ImgInterface(\n",
    "#         model_id, \n",
    "#         vae, \n",
    "#         unet, \n",
    "#         text_encoder, \n",
    "#         tokenizer,\n",
    "#         safety_checker = None,\n",
    "#         feature_extractor = None,\n",
    "#         scheduler = schedulers['DDIMScheduler'],  \n",
    "#         use_auth_token = HF_TOKEN,\n",
    "#     )\n",
    "\n",
    "# cycle_diffusion_interface = CycleDiffusionInterface(\n",
    "#     model_id, \n",
    "#     vae, \n",
    "#     unet, \n",
    "#     text_encoder, \n",
    "#     tokenizer,\n",
    "#     safety_checker = None,\n",
    "#     feature_extractor = None,\n",
    "#     scheduler = schedulers[\"PNDMScheduler\"],  \n",
    "#     use_auth_token = HF_TOKEN,\n",
    "# )\n",
    "# cycle_diffusion_interface = None\n",
    "\n",
    "\n",
    "\n",
    "# stable_diffusion_interface = CycleDiffusionInterface(\n",
    "#             model_id, \n",
    "#             vae, \n",
    "#             unet, \n",
    "#             text_encoder, \n",
    "#             tokenizer,\n",
    "#             safety_checker = None,\n",
    "#             feature_extractor = None,\n",
    "#             scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\"),  \n",
    "#             use_auth_token = HF_TOKEN,\n",
    "#         )\n",
    "\n",
    "\n",
    "# images = stable_diffusion_interface(\n",
    "#         prompt = \"\",\n",
    "#         source_prompt = \"\",\n",
    "#         image = Image.open(\"C:\\\\Users\\\\Rustam\\\\Downloads\\\\2023-03-26 04-02-23.913974.jpg\"),\n",
    "#         height = 384,\n",
    "#         width = 384,\n",
    "#         num_images_per_prompt = 1,\n",
    "#         num_inference_steps = 30,\n",
    "#         eta = 0.1,\n",
    "#         strength = 0.8,\n",
    "#         guidance_scale =7.5,\n",
    "#         source_guidance_scale = 1,\n",
    "#         seed = None,\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "save_path = \"\"\n",
    "\n",
    "def save_images(images, save_path):       \n",
    "    try:\n",
    "        for image in images:\n",
    "            # display(image)\n",
    "            if len(save_path) > 0:\n",
    "                curr_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f').replace(\":\", \"-\") #pytz.timezone('Europe/Moscow')\n",
    "                image.save(f\"{save_path}/{curr_date}.jpg\")\n",
    "    except:\n",
    "        print(\"Couldn't save image\")\n",
    "\n",
    "def save_prompts(prompt, neg_prompt, style_name):\n",
    "    filename = 'prompts.json'\n",
    "\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            existing_prompts = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        existing_prompts = {\"prompts\": []}\n",
    "\n",
    "    if style_name == \"\":\n",
    "        curr_date = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f').replace(\":\", \"-\")\n",
    "        style_name = f\"default_{curr_date}\"\n",
    "    \n",
    "    styles_list = [style[\"style_name\"] for style in existing_prompts[\"prompts\"]]\n",
    "    if style_name in styles_list:\n",
    "        existing_prompts[\"prompts\"][styles_list.index(style_name)] = {\"prompt\": prompt, \"neg_prompt\": neg_prompt, \"style_name\": style_name}\n",
    "    else:\n",
    "        existing_prompts[\"prompts\"].append({\"prompt\": prompt, \"neg_prompt\": neg_prompt, \"style_name\": style_name})\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(existing_prompts, f, indent=4)\n",
    "\n",
    "    return gr.Dropdown.update(choices = get_styles(), value = \"\")\n",
    "\n",
    "def load_prompts(style_name):\n",
    "    filename = 'prompts.json'\n",
    "\n",
    "    \n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            existing_prompts = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return [\n",
    "            gr.update(),\n",
    "            gr.update(),\n",
    "        ]\n",
    "  \n",
    "    styles_list = [style[\"style_name\"] for style in existing_prompts[\"prompts\"]]\n",
    "\n",
    "    if style_name not in styles_list:\n",
    "        return [\n",
    "            gr.update(),\n",
    "            gr.update(),\n",
    "        ]\n",
    "    return [\n",
    "        gr.update(value = existing_prompts[\"prompts\"][styles_list.index(style_name)][\"prompt\"]),\n",
    "        gr.update(value = existing_prompts[\"prompts\"][styles_list.index(style_name)][\"neg_prompt\"])\n",
    "    ]\n",
    "\n",
    "def get_styles(filename = 'prompts.json'):\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            existing_prompts = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        existing_prompts = {\"prompts\": []}\n",
    "    styles_list = [style[\"style_name\"] for style in existing_prompts[\"prompts\"]]\n",
    "    return styles_list\n",
    "\n",
    "\n",
    "\n",
    "def generate_images(\n",
    "    model_id,\n",
    "    prompt, \n",
    "    negative_prompt = \"\", \n",
    "    num_samples = 1, \n",
    "    guidance_scale = 7.5, \n",
    "    num_inference_steps = 25, \n",
    "    height = 512, \n",
    "    width = 512, \n",
    "    seed = None, \n",
    "    save_path = \"\",\n",
    "    scheduler_name = \"EulerAncestralDiscreteScheduler\",\n",
    "):\n",
    "\n",
    "    seed = None if int(seed) == -1 else abs(int(seed))\n",
    "\n",
    "    global stable_diffusion_interface\n",
    "    if ray.get(stable_diffusion_interface.name.remote()) != \"StableDiffusionInterface\" or ray.get(stable_diffusion_interface.model_name.remote()) != model_id:\n",
    "        ray.kill(stable_diffusion_interface)\n",
    "        stable_diffusion_interface = StableDiffusionInterface.remote(\n",
    "            model_id,\n",
    "            use_auth_token = HF_TOKEN,\n",
    "        )\n",
    "    ray.get(stable_diffusion_interface.set_scheduler.remote(schedulers[scheduler_name]))\n",
    "\n",
    "    images = ray.get(stable_diffusion_interface.__call__.remote(\n",
    "        prompt,\n",
    "        negative_prompt = negative_prompt,\n",
    "        height = height,\n",
    "        width = width,\n",
    "        num_images_per_prompt = num_samples,\n",
    "        num_inference_steps = num_inference_steps,\n",
    "        guidance_scale =guidance_scale,\n",
    "        seed = seed,\n",
    "    ))\n",
    "\n",
    "    for image in images:\n",
    "        if type(image) is np.ndarray:\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "    save_images(images, save_path)\n",
    "    return images\n",
    "\n",
    "\n",
    "def cycle_generate_images(\n",
    "    model_id,\n",
    "    prompt= \"\", \n",
    "    source_prompt = \"\",\n",
    "    num_samples = 1, \n",
    "    eta=0.1,\n",
    "    strength=0.85,\n",
    "    guidance_scale = 7.5, \n",
    "    source_guidance_scale=1,\n",
    "    num_inference_steps = 25, \n",
    "    image = None,\n",
    "    height = 512, \n",
    "    width = 512, \n",
    "    seed = None, \n",
    "    save_path = \"\",\n",
    "    scheduler_name = \"DDIMScheduler\", \n",
    "):\n",
    "    seed = None if int(seed) == -1 else abs(int(seed))\n",
    "\n",
    "    image = image['image']\n",
    "    if type(image) is not Image.Image:\n",
    "        print(\"Cycle Diffusion Pipeline: Couldn't open image\")\n",
    "\n",
    "        return None       \n",
    "\n",
    "    global stable_diffusion_interface\n",
    "    if ray.get(stable_diffusion_interface.name.remote()) != \"CycleDiffusionInterface\" or ray.get(stable_diffusion_interface.model_name.remote()) != model_id:\n",
    "        ray.kill(stable_diffusion_interface)\n",
    "        \n",
    "        stable_diffusion_interface = CycleDiffusionInterface.remote(\n",
    "            model_id,\n",
    "            use_auth_token = HF_TOKEN,\n",
    "        )\n",
    "    ray.get(stable_diffusion_interface.set_scheduler.remote(schedulers[scheduler_name]))\n",
    "    \n",
    "    images = ray.get(stable_diffusion_interface.__call__.remote(\n",
    "        prompt = prompt,\n",
    "        source_prompt = source_prompt,\n",
    "        image = image,\n",
    "        height = height,\n",
    "        width = width,\n",
    "        num_images_per_prompt = num_samples,\n",
    "        num_inference_steps = num_inference_steps,\n",
    "        eta = eta,\n",
    "        strength = strength,\n",
    "        guidance_scale =guidance_scale,\n",
    "        source_guidance_scale = source_guidance_scale,\n",
    "        seed = seed,\n",
    "    ))\n",
    "\n",
    "    for image in images:\n",
    "        if type(image) is np.ndarray:\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "    save_images(images, save_path)\n",
    "    return images\n",
    "\n",
    "def img2img_generate_images(\n",
    "    model_id,\n",
    "    prompt = \"\",\n",
    "    negative_prompt = \"\",\n",
    "    num_samples = 1,\n",
    "    guidance_scale = 7.5,\n",
    "    eta = 0.1,\n",
    "    strength = 0.85,\n",
    "    num_inference_steps = 25,\n",
    "    image = None,\n",
    "    height = 512,\n",
    "    width = 512,\n",
    "    seed = None,\n",
    "    save_path = \"\",\n",
    "    scheduler_name = \"DDIMScheduler\",\n",
    "):\n",
    "    seed = None if int(seed) == -1 else abs(int(seed))\n",
    "\n",
    "    image = image['image']\n",
    "    if type(image) is not Image.Image:\n",
    "        print(\"Img2Img Pipeline: Couldn't open image\")\n",
    "        return None\n",
    "\n",
    "    global stable_diffusion_interface\n",
    "    if ray.get(stable_diffusion_interface.name.remote()) != \"Img2ImgInterface\" or ray.get(stable_diffusion_interface.model_name.remote()) != model_id:\n",
    "        ray.kill(stable_diffusion_interface)\n",
    "        \n",
    "        stable_diffusion_interface = Img2ImgInterface.remote(\n",
    "            model_id,\n",
    "            use_auth_token = HF_TOKEN,\n",
    "        )\n",
    "    ray.get(stable_diffusion_interface.set_scheduler.remote(schedulers[scheduler_name]))\n",
    "    \n",
    "    images = ray.get(stable_diffusion_interface.__call__.remote(\n",
    "        prompt = prompt,\n",
    "        negative_prompt = negative_prompt,\n",
    "        image = image,\n",
    "        height = height,\n",
    "        width = width,\n",
    "        num_images_per_prompt = num_samples,\n",
    "        num_inference_steps = num_inference_steps,\n",
    "        eta = eta,\n",
    "        strength = strength,\n",
    "        guidance_scale = guidance_scale,\n",
    "        seed = seed,\n",
    "    ))\n",
    "\n",
    "    for image in images:\n",
    "        if type(image) is np.ndarray:\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "    save_images(images, save_path)\n",
    "    return images\n",
    "\n",
    "def inpaint_generate_images(\n",
    "    model_id,\n",
    "    prompt = \"\",\n",
    "    negative_prompt = \"\",\n",
    "    num_samples = 1,\n",
    "    guidance_scale = 7.5,\n",
    "    eta = 0.1,\n",
    "    num_inference_steps = 25,\n",
    "    image = None,\n",
    "    height = 512,\n",
    "    width = 512,\n",
    "    seed = None,\n",
    "    save_path = \"\",\n",
    "    scheduler_name = \"DDIMScheduler\",\n",
    "):\n",
    "    seed = None if int(seed) == -1 else abs(int(seed))\n",
    "\n",
    "    image, mask_image = image['image'], image['mask']\n",
    "    if type(image) is not Image.Image:\n",
    "        print(\"Inpaint Pipeline: Couldn't open image\")\n",
    "        return None\n",
    " \n",
    "    global stable_diffusion_interface\n",
    "    if ray.get(stable_diffusion_interface.name.remote()) != \"InpaintInterface\" or ray.get(stable_diffusion_interface.model_name.remote()) != model_id:\n",
    "        ray.kill(stable_diffusion_interface)\n",
    "        \n",
    "        stable_diffusion_interface = InpaintInterface.remote(\n",
    "            model_id,\n",
    "            use_auth_token = HF_TOKEN,\n",
    "        )\n",
    "    ray.get(stable_diffusion_interface.set_scheduler.remote(schedulers[scheduler_name]))\n",
    "    \n",
    "    images = ray.get(stable_diffusion_interface.__call__.remote(\n",
    "        prompt = prompt,\n",
    "        negative_prompt = negative_prompt,\n",
    "        image = image,\n",
    "        mask_image = mask_image,\n",
    "        height = height,\n",
    "        width = width,\n",
    "        num_images_per_prompt = num_samples,\n",
    "        num_inference_steps = num_inference_steps,\n",
    "        eta = eta,\n",
    "        guidance_scale = guidance_scale,\n",
    "        seed = seed,\n",
    "\n",
    "    ))\n",
    "\n",
    "    for image in images:\n",
    "        if type(image) is np.ndarray:\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "    save_images(images, save_path)\n",
    "    return images\n",
    "\n",
    "def depth2img_generate_images(\n",
    "    model_id,   \n",
    "    prompt = \"\",\n",
    "    negative_prompt = \"\",\n",
    "    num_samples = 1,\n",
    "    guidance_scale = 7.5,\n",
    "    eta = 0.1,\n",
    "    strength = 0.85,\n",
    "    num_inference_steps = 25,\n",
    "    image = None,\n",
    "    height = 512,\n",
    "    width = 512,\n",
    "    seed = None,\n",
    "    save_path = \"\",\n",
    "    scheduler_name = \"DDIMScheduler\"\n",
    "):\n",
    "    seed = None if int(seed) == -1 else abs(int(seed))\n",
    "\n",
    "    image, mask_image = image['image'], image['mask']\n",
    "    if type(image) is not Image.Image:\n",
    "        print(\"Depth2img Pipeline: Couldn't open image\")\n",
    "        return None\n",
    "    \n",
    "    global stable_diffusion_interface\n",
    "    if ray.get(stable_diffusion_interface.name.remote()) != \"Depth2ImgInterface\" or ray.get(stable_diffusion_interface.model_name.remote()) != model_id:\n",
    "        ray.kill(stable_diffusion_interface)\n",
    "        \n",
    "        stable_diffusion_interface = Depth2ImgInterface.remote(\n",
    "            model_id,\n",
    "            use_auth_token = HF_TOKEN,\n",
    "        )\n",
    "    ray.get(stable_diffusion_interface.set_scheduler.remote(schedulers[scheduler_name]))\n",
    "\n",
    "    images = ray.get(stable_diffusion_interface.__call__.remote(\n",
    "        prompt = prompt,\n",
    "        negative_prompt = negative_prompt,\n",
    "        image = image,\n",
    "        height = height,\n",
    "        width = width,\n",
    "        num_images_per_prompt = num_samples,\n",
    "        num_inference_steps = num_inference_steps,\n",
    "        eta = eta,\n",
    "        strength = strength,\n",
    "        guidance_scale = guidance_scale,\n",
    "        seed = seed,\n",
    "\n",
    "    ))\n",
    "\n",
    "    for image in images:\n",
    "        if type(image) is np.ndarray:\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "    save_images(images, save_path)\n",
    "    return images\n",
    "    \n",
    "\n",
    "def select_interface(interface_name):\n",
    "    if interface_name == \"Stable Diffusion pipeline\":\n",
    "        return {\n",
    "            negative_prompt: gr.update(visible=True),\n",
    "            source_prompt: gr.update(visible=False),\n",
    "            scheduler: gr.update(choices=[\n",
    "                \"EulerAncestralDiscreteScheduler\",\n",
    "                \"EulerDiscreteScheduler\",\n",
    "                \"DDIMScheduler\",\n",
    "                \"DPMSolverMultistepScheduler\",\n",
    "                \"LMSDiscreteScheduler\",\n",
    "                \"PNDMScheduler\"\n",
    "                ],\n",
    "                value=\"EulerAncestralDiscreteScheduler\", \n",
    "            ),\n",
    "            eta: gr.update(visible=False),\n",
    "            strength: gr.update(visible=False),\n",
    "            source_guidance_scale: gr.update(visible=False),\n",
    "            image_input: gr.update(visible=False),\n",
    "            generate_button: gr.update(visible=True),\n",
    "            cycle_generate_button: gr.update(visible=False),\n",
    "            img2img_generate_button: gr.update(visible=False),\n",
    "            inpaint_generate_button: gr.update(visible=False),\n",
    "            depth2img_generate_button: gr.update(visible=False),\n",
    "        }\n",
    "\n",
    "    elif interface_name == \"Cycle Diffusion pipeline\":\n",
    "        return {\n",
    "            negative_prompt: gr.update(visible=False),\n",
    "            source_prompt: gr.update(visible=True),\n",
    "            scheduler: gr.update(choices=[\n",
    "                \"DDIMScheduler\",\n",
    "                ],\n",
    "                value=\"DDIMScheduler\",\n",
    "            ),\n",
    "            eta: gr.update(visible=True),\n",
    "            strength: gr.update(visible=True),\n",
    "            source_guidance_scale: gr.update(visible=True),\n",
    "            image_input: gr.update(visible=True),\n",
    "            generate_button: gr.update(visible=False),\n",
    "            cycle_generate_button: gr.update(visible=True),\n",
    "            img2img_generate_button: gr.update(visible=False),\n",
    "            inpaint_generate_button: gr.update(visible=False),\n",
    "            depth2img_generate_button: gr.update(visible=False),\n",
    "        }\n",
    "    elif interface_name == \"Img2Img Pipeline\":\n",
    "        return {\n",
    "            negative_prompt: gr.update(visible=True),\n",
    "            source_prompt: gr.update(visible=False),\n",
    "            scheduler: gr.update(choices=[\n",
    "                \"EulerAncestralDiscreteScheduler\",\n",
    "                \"EulerDiscreteScheduler\",\n",
    "                \"DDIMScheduler\",\n",
    "                \"DPMSolverMultistepScheduler\",\n",
    "                \"LMSDiscreteScheduler\",\n",
    "                \"PNDMScheduler\"\n",
    "                ],\n",
    "                value=\"EulerAncestralDiscreteScheduler\", \n",
    "            ),\n",
    "            eta: gr.update(visible=True),\n",
    "            strength: gr.update(visible=True),\n",
    "            source_guidance_scale: gr.update(visible=False),\n",
    "            image_input: gr.update(visible=True),\n",
    "            generate_button: gr.update(visible=False),\n",
    "            cycle_generate_button: gr.update(visible=False),\n",
    "            img2img_generate_button: gr.update(visible=True),\n",
    "            inpaint_generate_button: gr.update(visible=False),\n",
    "            depth2img_generate_button: gr.update(visible=False),\n",
    "        }\n",
    "    elif interface_name == \"Inpaint Pipeline\":\n",
    "        return {\n",
    "            negative_prompt: gr.update(visible=True),\n",
    "            source_prompt: gr.update(visible=False),\n",
    "            scheduler: gr.update(choices=[\n",
    "                \"EulerAncestralDiscreteScheduler\",\n",
    "                \"EulerDiscreteScheduler\",\n",
    "                \"DDIMScheduler\",\n",
    "                \"DPMSolverMultistepScheduler\",\n",
    "                \"LMSDiscreteScheduler\",\n",
    "                \"PNDMScheduler\"\n",
    "                ],\n",
    "                value=\"EulerAncestralDiscreteScheduler\", \n",
    "            ),\n",
    "            eta: gr.update(visible=True),\n",
    "            strength: gr.update(visible=False),\n",
    "            source_guidance_scale: gr.update(visible=False),\n",
    "            image_input: gr.update(visible=True),\n",
    "            generate_button: gr.update(visible=False),\n",
    "            cycle_generate_button: gr.update(visible=False),\n",
    "            img2img_generate_button: gr.update(visible=False),\n",
    "            inpaint_generate_button: gr.update(visible=True),\n",
    "            depth2img_generate_button: gr.update(visible=False),\n",
    "        } \n",
    "    elif interface_name == \"Depth2Img Pipeline\":\n",
    "        return {\n",
    "            negative_prompt: gr.update(visible=True),\n",
    "            source_prompt: gr.update(visible=False),\n",
    "            scheduler: gr.update(choices=[\n",
    "                \"EulerAncestralDiscreteScheduler\",\n",
    "                \"EulerDiscreteScheduler\",\n",
    "                \"DDIMScheduler\",\n",
    "                \"DPMSolverMultistepScheduler\",\n",
    "                \"LMSDiscreteScheduler\",\n",
    "                \"PNDMScheduler\"\n",
    "                ],\n",
    "                value=\"EulerAncestralDiscreteScheduler\", \n",
    "            ),\n",
    "            eta: gr.update(visible=True),\n",
    "            strength: gr.update(visible=True),\n",
    "            source_guidance_scale: gr.update(visible=False),\n",
    "            image_input: gr.update(visible=True),\n",
    "            generate_button: gr.update(visible=False),\n",
    "            cycle_generate_button: gr.update(visible=False),\n",
    "            img2img_generate_button: gr.update(visible=False),\n",
    "            inpaint_generate_button: gr.update(visible=False),\n",
    "            depth2img_generate_button: gr.update(visible=True),\n",
    "        }\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"Hugging Face Stable Diffusion\")\n",
    "\n",
    "    with gr.Tab(\"Main\"):  \n",
    "        interfaces_box = gr.Radio(\n",
    "            label=\"Interface\", \n",
    "            choices = [\n",
    "                \"Stable Diffusion pipeline\", \n",
    "                \"Cycle Diffusion pipeline\",\n",
    "                \"Img2Img Pipeline\",\n",
    "                \"Inpaint Pipeline\",\n",
    "                \"Depth2Img Pipeline\",\n",
    "            ], \n",
    "            value = \"Stable Diffusion pipeline\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale = 0.7):\n",
    "                prompt = gr.Textbox(label=\"Prompt\", lines = 3)\n",
    "                \n",
    "                source_prompt = gr.Textbox(label=\"Source Prompt\", visible=False, lines = 3)\n",
    "                negative_prompt = gr.Textbox(label=\"Negative Prompt\", visible=True, lines = 3)\n",
    "                \n",
    "            with gr.Column(scale = 0.3):\n",
    "                prompts_list = gr.Dropdown(\n",
    "                    choices = get_styles(), label=\"Styles\"\n",
    "                    )\n",
    "                load_prompts_button = gr.Button(\"Load Style\")\n",
    "                save_prompts_name = gr.Textbox(label=\"Save Style Name\")\n",
    "                save_prompts_button = gr.Button(\"Save Style\")\n",
    "\n",
    "\n",
    "        num_samples = gr.Slider(label=\"Number of samples\", value = 1, step = 1, minimum = 1, maximum = 4)\n",
    "        height = gr.Slider(label=\"Height\", value = 512, step = 64, minimum = 64, maximum = 1024)\n",
    "        width = gr.Slider(label=\"Width\", value = 512, step = 64, minimum = 64, maximum = 1024)\n",
    "\n",
    "        eta = gr.Slider(label=\"Eta\", value = 0.1, step = 0.1, minimum = 0.1, maximum = 1.0, visible=False)\n",
    "        strength = gr.Slider(label=\"Strength\", value = 0.85, step = 0.05, minimum = 0.05, maximum = 1.0, visible=False)\n",
    "        \n",
    "        with gr.Row():\n",
    "            \n",
    "            with gr.Column():\n",
    "                scheduler = gr.Dropdown(\n",
    "                    label=\"Scheduler\", \n",
    "                    choices=[\n",
    "                        \"EulerAncestralDiscreteScheduler\", \n",
    "                        \"EulerDiscreteScheduler\", \n",
    "                        \"DDIMScheduler\",\n",
    "                        \"DPMSolverMultistepScheduler\", \n",
    "                        \"LMSDiscreteScheduler\", \n",
    "                        \"PNDMScheduler\"\n",
    "                    ],\n",
    "                    value = \"EulerAncestralDiscreteScheduler\"\n",
    "                )\n",
    "                num_inference_steps = gr.Slider(label=\"Number of inference steps\", value = 25, step = 1, minimum = 1, maximum = 100)\n",
    "                guidance_scale = gr.Slider(label=\"Guidance scale\", value = 7.5, step = 0.1, minimum = 0.1, maximum = 10.0)\n",
    "                source_guidance_scale = gr.Slider(label=\"Source guidance scale\", value = 1, step = 0.1, minimum = 0.1, maximum = 10.0, visible=False)\n",
    "                \n",
    "                seed = gr.Number(label=\"Seed\", value = -1)\n",
    "                generate_button = gr.Button(\"Generate\")\n",
    "                cycle_generate_button = gr.Button(\"Generate\", visible=False)\n",
    "                img2img_generate_button = gr.Button(\"Generate\", visible=False)\n",
    "                inpaint_generate_button = gr.Button(\"Generate\", visible=False)\n",
    "                depth2img_generate_button = gr.Button(\"Generate\", visible=False)\n",
    "\n",
    "            image_input = gr.Image(type=\"pil\", tool='sketch', visible=False)\n",
    "            image_output = gr.Gallery(show_label=False).style(grid=[2], height=\"auto\", preview = True)\n",
    "\n",
    "    with gr.Tab(\"Pathes\"):\n",
    "        with gr.Column():\n",
    "            \n",
    "            model_identifier = gr.Textbox(label=\"Model id or path\", value = model_id)\n",
    "            inpaint_model_identifier = gr.Textbox(label=\"Inpaint Model id or path\", value = inpaint_model_id)\n",
    "            depth2img_model_identifier = gr.Textbox(label=\"Depth2Img Model id or path\", value = depth2img_model_id)\n",
    "\n",
    "            save_path = gr.Textbox(label=\"Images save path\", value = save_path)\n",
    "\n",
    "            \n",
    "    interfaces_box.change(\n",
    "        select_interface, \n",
    "        interfaces_box, \n",
    "        [\n",
    "            negative_prompt, \n",
    "            source_prompt, \n",
    "            scheduler,\n",
    "            eta,\n",
    "            strength,\n",
    "            source_guidance_scale,\n",
    "            image_input,\n",
    "            generate_button,\n",
    "            cycle_generate_button,\n",
    "            img2img_generate_button,\n",
    "            inpaint_generate_button,\n",
    "            depth2img_generate_button\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    generate_button.click(\n",
    "        generate_images, \n",
    "        inputs = [\n",
    "            model_identifier,\n",
    "            prompt, \n",
    "            negative_prompt,\n",
    "            num_samples,\n",
    "            guidance_scale,\n",
    "            num_inference_steps,\n",
    "            height,\n",
    "            width,\n",
    "            seed,\n",
    "            save_path,\n",
    "            scheduler\n",
    "        ], \n",
    "        outputs=image_output\n",
    "    )\n",
    "\n",
    "    cycle_generate_button.click(\n",
    "        cycle_generate_images,\n",
    "        inputs = [\n",
    "            model_identifier,\n",
    "            prompt,\n",
    "            source_prompt,\n",
    "            num_samples,\n",
    "            eta,\n",
    "            strength,\n",
    "            guidance_scale,\n",
    "            source_guidance_scale,\n",
    "            num_inference_steps,\n",
    "            image_input,\n",
    "            height,\n",
    "            width,\n",
    "            seed,\n",
    "            save_path,\n",
    "            scheduler\n",
    "        \n",
    "        ],\n",
    "        outputs=image_output\n",
    "    )\n",
    "\n",
    "    img2img_generate_button.click(\n",
    "        img2img_generate_images,\n",
    "        inputs = [\n",
    "            model_identifier,\n",
    "            prompt,\n",
    "            negative_prompt,\n",
    "            num_samples,\n",
    "            guidance_scale,\n",
    "            eta,\n",
    "            strength,\n",
    "            num_inference_steps,\n",
    "            image_input,\n",
    "            height,\n",
    "            width,\n",
    "            seed,\n",
    "            save_path,\n",
    "            scheduler\n",
    "        ],\n",
    "        outputs=image_output\n",
    "    )\n",
    "\n",
    "    inpaint_generate_button.click(\n",
    "        inpaint_generate_images,\n",
    "        inputs = [\n",
    "            inpaint_model_identifier,\n",
    "            prompt,\n",
    "            negative_prompt,\n",
    "            num_samples,\n",
    "            guidance_scale,\n",
    "            eta,\n",
    "            num_inference_steps,\n",
    "            image_input,\n",
    "            height,\n",
    "            width,\n",
    "            seed,\n",
    "            save_path,\n",
    "            scheduler\n",
    "        ],\n",
    "        outputs=image_output\n",
    "    )\n",
    "\n",
    "    depth2img_generate_button.click(\n",
    "        depth2img_generate_images,\n",
    "        inputs = [\n",
    "            depth2img_model_identifier,\n",
    "            prompt,\n",
    "            negative_prompt,\n",
    "            num_samples,\n",
    "            guidance_scale,\n",
    "            eta,\n",
    "            strength,\n",
    "            num_inference_steps,\n",
    "            image_input,\n",
    "            height,\n",
    "            width,\n",
    "            seed,\n",
    "            save_path,\n",
    "            scheduler\n",
    "        ],\n",
    "        outputs=image_output\n",
    "    )\n",
    "\n",
    "    save_prompts_button.click(\n",
    "        save_prompts,\n",
    "        inputs = [\n",
    "            prompt,\n",
    "            negative_prompt,\n",
    "            save_prompts_name,\n",
    "        ],\n",
    "        outputs = prompts_list\n",
    "    )\n",
    "\n",
    "    load_prompts_button.click(\n",
    "        load_prompts,\n",
    "        inputs = [\n",
    "            prompts_list,\n",
    "        ],\n",
    "        outputs = [\n",
    "            prompt,\n",
    "            negative_prompt\n",
    "        ]\n",
    "    )\n",
    "\n",
    "demo.queue()\n",
    "demo.launch(share = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85ad5e3f658df10a348b2c4bce0efb8f62276ea26564ae3f712af45ed083858c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
